{"version":3,"sources":["../../../lib/compiler/tokenizer.js"],"names":["tokens","matchPattern","require","Text","getTopLevelTokens","Object","keys","map","key","filter","token","priority","sort","a","b","removeExtraCloses","input","remove","Set","closeSubject","opens","closes","expectedSubjects","forEach","index","tokenType","startsWith","push","subject","path","test","raw","expectedSubject","pop","add","matches","match","i","length","tok","m","diff","console","warn","output","has","tokenizer","topLevelTokens","cursor","lastBreak","slice","found","text","escapedText","Tok","module","exports"],"mappings":"AAAA;;AAEA,MAAM,EAAEA,MAAF,EAAUC,YAAV,KAA2BC,QAAQ,UAAR,CAAjC;;AAEA,MAAM,EAAEC,IAAF,KAAWH,MAAjB;;AAEA,SAASI,iBAAT,GAA6B;AAC3B,SAAOC,OAAOC,IAAP,CAAYN,MAAZ,EACJO,GADI,CACAC,OAAOR,OAAOQ,GAAP,CADP,EAEJC,MAFI,CAEGC,SAASA,MAAMC,QAAN,GAAiB,CAF7B,EAGJC,IAHI,CAGC,CAACC,CAAD,EAAIC,CAAJ,KAAUD,EAAEF,QAAF,GAAaG,EAAEH,QAH1B,CAAP;AAID;;AAED,SAASI,iBAAT,CAA2BC,KAA3B,EAAkC;AAChC,QAAMC,SAAS,IAAIC,GAAJ,EAAf;AACA,QAAMC,eAAe,4BAArB;;AAEA,MAAIC,QAAQ,CAAZ;AACA,MAAIC,SAAS,CAAb;;AAEA,QAAMC,mBAAmB,EAAzB;AACA;AACAN,QAAMO,OAAN,CAAc,CAACb,KAAD,EAAQc,KAAR,KAAkB;AAC9B,QAAId,MAAMe,SAAN,CAAgBC,UAAhB,CAA2B,MAA3B,CAAJ,EAAwC;AACtCN,eAAS,CAAT;;AAEAE,uBAAiBK,IAAjB,CACGjB,MAAMkB,OAAN,IAAiBlB,MAAMkB,OAAN,CAAcC,IAAhC,IACCnB,MAAMoB,IAAN,KAAepB,MAAMoB,IAAN,CAAWC,GAAX,IAAkBrB,MAAMoB,IAAN,CAAWD,IAA5C,CAFH;AAID,KAPD,MAOO,IAAInB,MAAMe,SAAN,KAAoB,OAAxB,EAAiC;AACtCJ,gBAAU,CAAV;;AAEA,YAAMW,kBAAkBV,iBAAiBW,GAAjB,EAAxB;;AAEA,UAAI,CAACD,eAAL,EAAsB;AACpBf,eAAOiB,GAAP,CAAWxB,KAAX;AACD,OAFD,MAEO;AACL,cAAMyB,UAAUzB,MAAMqB,GAAN,CAAUK,KAAV,CAAgBjB,YAAhB,CAAhB;AACA,YAAIgB,WAAW,CAACH,gBAAgBN,UAAhB,CAA2BS,QAAQ,CAAR,CAA3B,CAAhB,EAAwD;AACtDlB,iBAAOiB,GAAP,CAAWxB,KAAX;AACAY,2BAAiBK,IAAjB,CAAsBK,eAAtB;AACD,SAHD,MAGO;AACL;AACA;AACA,eAAK,IAAIK,IAAIb,QAAQ,CAArB,EAAwBa,IAAIrB,MAAMsB,MAAlC,EAA0CD,KAAK,CAA/C,EAAkD;AAChD,kBAAME,MAAMvB,MAAMqB,CAAN,CAAZ;AACA,gBAAIE,IAAId,SAAJ,CAAcC,UAAd,CAAyB,MAAzB,CAAJ,EAAsC;AACpC;AACD;AACD,gBAAIa,IAAId,SAAJ,KAAkB,OAAtB,EAA+B;AAC7B,oBAAMe,IAAID,IAAIR,GAAJ,CAAQK,KAAR,CAAcjB,YAAd,CAAV;AACA,kBAAIqB,KAAKA,EAAE,CAAF,MAASR,eAAlB,EAAmC;AACjC;AACAf,uBAAOiB,GAAP,CAAWxB,KAAX;AACAY,iCAAiBK,IAAjB,CAAsBK,eAAtB;AACA;AACD;AACF;AACF;AACF;AACF;AACF;AACF,GAzCD;;AA2CA,MAAIX,SAASD,KAAb,EAAoB;AAClB,QAAIqB,OAAOpB,SAASD,KAApB;;AAEA;AACAsB,YAAQC,IAAR,CAAa,uBAAb;;AAEA,UAAMC,SAAS5B,MAAMT,GAAN,CAAWG,KAAD,IAAW;AAClC,UAAIO,OAAO4B,GAAP,CAAWnC,KAAX,KAAqB+B,OAAO,CAAhC,EAAmC;AACjCC,gBAAQC,IAAR,CAAajC,MAAMqB,GAAnB;;AAEAU,gBAAQ,CAAR;AACA,eAAO,IAAItC,IAAJ,CAASO,MAAMqB,GAAf,CAAP;AACD;;AAED,aAAOrB,KAAP;AACD,KATc,CAAf;;AAWAgC,YAAQC,IAAR,CAAa,0GAAb;AACA;;AAEA,WAAOC,MAAP;AACD;;AAED,SAAO5B,KAAP;AACD;;AAED;;;;;AAKA,SAAS8B,SAAT,CAAmB9B,KAAnB,EAA0B;AACxB,QAAM+B,iBAAiB3C,mBAAvB;;AAEA,QAAMkC,SAAStB,MAAMsB,MAArB;;AAEA,QAAMM,SAAS,EAAf;;AAEA,MAAII,SAAS,CAAb;AACA,MAAIC,YAAY,CAAhB;;AAEA,SAAOD,SAASV,MAAhB,EAAwB;AACtB,UAAMY,QAAQlC,MAAMkC,KAAN,CAAYF,MAAZ,CAAd;AACA,UAAMG,QAAQlD,aAAa8C,cAAb,EAA6BG,KAA7B,EAAoC,KAApC,CAAd;;AAEA,QAAIC,SAASnC,MAAMgC,SAAS,CAAf,MAAsB,IAAnC,EAAyC;AACvC,YAAMI,OAAOpC,MAAMkC,KAAN,CAAYD,SAAZ,EAAuBD,SAAS,CAAhC,CAAb;AACA,UAAII,IAAJ,EAAU;AACRR,eAAOjB,IAAP,CAAY,IAAIxB,IAAJ,CAASiD,IAAT,CAAZ;AACD;;AAED,YAAMC,cAAcF,MAAM,CAAN,EAAS,CAAT,CAApB;AACAP,aAAOjB,IAAP,CAAY,IAAIxB,IAAJ,CAASkD,WAAT,CAAZ;;AAEAL,gBAAUK,YAAYf,MAAtB;AACAW,kBAAYD,MAAZ;AACD,KAXD,MAWO,IAAIG,KAAJ,EAAW;AAChB,YAAM,CAACG,GAAD,EAAMnB,OAAN,IAAiBgB,KAAvB;;AAEA,YAAMC,OAAOpC,MAAMkC,KAAN,CAAYD,SAAZ,EAAuBD,MAAvB,CAAb;AACA,UAAII,IAAJ,EAAU;AACRR,eAAOjB,IAAP,CAAY,IAAIxB,IAAJ,CAASiD,IAAT,CAAZ;AACD;AACDR,aAAOjB,IAAP,CAAY,IAAI2B,GAAJ,CAAQ,GAAGnB,OAAX,CAAZ;;AAEAa,gBAAUb,QAAQ,CAAR,EAAWG,MAArB;AACAW,kBAAYD,MAAZ;AACD,KAXM,MAWA;AACLA,gBAAU,CAAV;AACD;AACF;AACD,QAAMI,OAAOpC,MAAMkC,KAAN,CAAYD,SAAZ,EAAuBD,MAAvB,CAAb;AACA,MAAII,IAAJ,EAAU;AACRR,WAAOjB,IAAP,CAAY,IAAIxB,IAAJ,CAASiD,IAAT,CAAZ;AACD;;AAED;AACA;AACA,SAAOrC,kBAAkB6B,MAAlB,CAAP;AACD;;AAEDW,OAAOC,OAAP,GAAiBV,SAAjB","file":"tokenizer.js","sourcesContent":["'use strict';\n\nconst { tokens, matchPattern } = require('./tokens');\n\nconst { Text } = tokens;\n\nfunction getTopLevelTokens() {\n  return Object.keys(tokens)\n    .map(key => tokens[key])\n    .filter(token => token.priority > 0)\n    .sort((a, b) => a.priority - b.priority);\n}\n\nfunction removeExtraCloses(input) {\n  const remove = new Set();\n  const closeSubject = /^<!-- END[^ ]* !?(.+) -->$/;\n\n  let opens = 0;\n  let closes = 0;\n\n  const expectedSubjects = [];\n  // try to find a Close with no corresponding Open\n  input.forEach((token, index) => {\n    if (token.tokenType.startsWith('Open')) {\n      opens += 1;\n\n      expectedSubjects.push(\n        (token.subject && token.subject.path) ||\n        (token.test && (token.test.raw || token.test.path))\n      );\n    } else if (token.tokenType === 'Close') {\n      closes += 1;\n\n      const expectedSubject = expectedSubjects.pop();\n\n      if (!expectedSubject) {\n        remove.add(token);\n      } else {\n        const matches = token.raw.match(closeSubject);\n        if (matches && !expectedSubject.startsWith(matches[1])) {\n          remove.add(token);\n          expectedSubjects.push(expectedSubject);\n        } else {\n          // search for a close within close proximity\n          // that has the expected subject\n          for (let i = index + 1; i < input.length; i += 1) {\n            const tok = input[i];\n            if (tok.tokenType.startsWith('Open')) {\n              break;\n            }\n            if (tok.tokenType === 'Close') {\n              const m = tok.raw.match(closeSubject);\n              if (m && m[1] === expectedSubject) {\n                // found one ahead, so remove the current one\n                remove.add(token);\n                expectedSubjects.push(expectedSubject);\n                break;\n              }\n            }\n          }\n        }\n      }\n    }\n  });\n\n  if (closes > opens) {\n    let diff = closes - opens;\n\n    /* eslint-disable no-console */\n    console.warn('Found extra token(s):');\n\n    const output = input.map((token) => {\n      if (remove.has(token) && diff > 0) {\n        console.warn(token.raw);\n\n        diff -= 1;\n        return new Text(token.raw);\n      }\n\n      return token;\n    });\n\n    console.warn('These tokens will be passed through as text, but you should remove them to prevent issues in the future.');\n    /* eslint-enable no-console */\n\n    return output;\n  }\n\n  return input;\n}\n\n/**\n * Generate an array of tokens describing the template\n * @param {string} input\n * @return {Token[]}\n */\nfunction tokenizer(input) {\n  const topLevelTokens = getTopLevelTokens();\n\n  const length = input.length;\n\n  const output = [];\n\n  let cursor = 0;\n  let lastBreak = 0;\n\n  while (cursor < length) {\n    const slice = input.slice(cursor);\n    const found = matchPattern(topLevelTokens, slice, false);\n\n    if (found && input[cursor - 1] === '\\\\') {\n      const text = input.slice(lastBreak, cursor - 1);\n      if (text) {\n        output.push(new Text(text));\n      }\n\n      const escapedText = found[1][0];\n      output.push(new Text(escapedText));\n\n      cursor += escapedText.length;\n      lastBreak = cursor;\n    } else if (found) {\n      const [Tok, matches] = found;\n\n      const text = input.slice(lastBreak, cursor);\n      if (text) {\n        output.push(new Text(text));\n      }\n      output.push(new Tok(...matches));\n\n      cursor += matches[0].length;\n      lastBreak = cursor;\n    } else {\n      cursor += 1;\n    }\n  }\n  const text = input.slice(lastBreak, cursor);\n  if (text) {\n    output.push(new Text(text));\n  }\n\n  // if there are more closes than opens\n  // intelligently remove extra ones\n  return removeExtraCloses(output);\n}\n\nmodule.exports = tokenizer;\n"]}